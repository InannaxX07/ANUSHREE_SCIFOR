{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7mxI3c/h3v1bNAY7x3CHD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InannaxX07/ANUSHREE_SCIFOR/blob/main/Naive_Bayes_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a powerful and versatile classification algorithm based on Bayes' theorem. It excels in situations where features are assumed to be conditionally independent given the class label. This assumption simplifies calculations and makes it computationally efficient, especially for large datasets.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "Features (Attributes): These represent the characteristics of each data point that contribute to the classification.\n",
        "\n",
        "Class Labels: The categories to which data points belong.\n",
        "\n",
        "Conditional Independence Assumption: Naive Bayes assumes that the presence or absence of one feature has no bearing on the existence or non-existence of another feature, given the class label.\n",
        "\n",
        "Implementation in Python Using scikit-learn:\n",
        "\n",
        "scikit-learn provides a well-optimized implementation of Naive Bayes, making it the preferred approach for most practical applications. Here's a step-by-step guide:\n",
        "\n",
        "1.Import Necessary Libraries\n",
        "\n",
        "2.Prepare Your Data:\n",
        "Ensure your data is in a NumPy array format, where each row represents a data point and each column represents a feature.\n",
        "If necessary, perform data preprocessing such as scaling or encoding categorical features.\n",
        "\n",
        "3.Split Data into Training and Testing Sets\n",
        "\n",
        "4.Choose the Appropriate Naive Bayes Classifier: GaussianNB: Use this for datasets with continuous features (e.g., numerical values).\n",
        "MultinomialNB: Use this for datasets with discrete features (e.g., text data, integer values).\n",
        "\n",
        "5.Create and Train the Model\n",
        "\n",
        "6.Make Predictions and Evaluate Performance\n"
      ],
      "metadata": {
        "id": "QKPRKCn3ewfV"
      }
    }
  ]
}
